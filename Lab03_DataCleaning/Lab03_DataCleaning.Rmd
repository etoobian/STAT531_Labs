
---
title: "Lab03: Data Cleaning with R"
author: "TECK Squad"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```

# Overview & Goals

In this lab, you will **continue working with the web advertising bids data** to:

- Systematically **detect** and **repair** some other data quality problems using those ideas.
- Create a **cleaned version** of the dataset (e.g., `bids_clean`).
- Document your work in a **data cleaning log**.

By the end, you should have:

1. A data set with the majority of the insidious issues corrected.
2. A short narrative (or table) explaining **what you fixed** and **how**.

> *Work in the **same repository** as previous labs. Update your Jira Kanban board with new cards for this lab and keep committing your progress with clear messages.*

![Kanban Planning Board](planning/KanbanBoard.PNG)

# Setup

## Packages

Minimally, you will need these packages:

```{r loadpackages}
library(tidyverse)
library(arrow)
library(dplyr)
library(knitr)
library(kableExtra)
library(stringr)
```

- `tidyverse` (especially `dplyr`, `ggplot2`, `readr`, `stringr`)
- `stringr` to modify character strings
- `lubridate` for date–time parsing
- `arrow` if you are reading/writing Parquet files

> **Task:** In a code chunk, load the packages you need.

## Load the messy bids data

Use the **same dataset** as in the earlier lab (e.g., a Parquet file in your `data/` folder).

> **Tasks:**
>
> 1. Read the messy data into R (e.g., using `arrow::read_parquet()` or `readr` functions, depending on your file).
> 2. Use `dplyr::glimpse()` and/or `summary()` to remind yourselves what variables are present and roughly what they look like.
> 3. Create a **working copy** of the data so that you can always go back to the original if needed.

You may want to keep the original as `bids_raw` and your working copy as `bids`.

```{r Read Data}
bids_raw <- read_parquet("../data/bids_data_vDTR.parquet")

```

# Quick Orientation Check (10–15 minutes)

Using your prior lab work and Data Cleaning lectures:

1. In **2–4 sentences**, describe **what each row** in this dataset represents.
2. Identify **one or two variables** that you think should be:
   - **Mandatory** (must not be missing), and
   - Possibly a **unique key** or part of a composite key.
3. Pick **two variables** and, for each, assign at least one **data quality dimension** (e.g., completeness, validity, consistency, uniqueness, cross-field consistency).

> **Write your answers directly in this Rmd in a text section (not code).**

# Fixing Numeric & Price Issues (30–40 minutes)

Focus on the **PRICE** variable (and other numeric variables if needed).

You should investigate:

- **Type issues** (e.g., stored as character instead of numeric),
- **Impossible values** (negative values, weird “huge” values),
- **Sentinel codes** (e.g., `-999`),
- Extra non-numeric characters.

## Suggested tools

- `class()`, `unique()`, `dplyr::count()`, `summary()`
- `readr::parse_number()`
- `as.numeric()`
- `dplyr::mutate()`, `dplyr::case_when()`
- `is.na()`, `sum(is.na(...))`
- `quantile()` to inspect distributions

## Inspect PRICE

> **Tasks:**
>
> 1. Check the **type** of `PRICE` and inspect a sample of values (e.g., using `sample()` or `head()`).

```{r}
# Your code here

```

> 2. In short sentences, describe what you see:
>    - Is `PRICE` numeric or character?
>    - Do you see obvious anomalies or strange values?


## Create a clean numeric version of PRICE

Create a **cleaned numeric version** (e.g., `PRICE_clean`) that:

- Extracts numeric content from strings (if necessary),
- Converts to numeric,
- Keeps track of values that cannot be converted (they will become `NA`).

> **Tasks:**
>
> 1. Use one or more of these:
>    - `readr::parse_number()`
>    - `stringr::str_remove_all()`
>    - `as.numeric()`
>
>    To learn about their use, type `?` before either of them (e.g., `?readr::parse_number()`)

```{r}
# Your code here

```

> 2. Count how many `NA`s you get in `PRICE_clean`. Explain why some values might be `NA` after parsing.


```{r}
# Your code here

```


## Define and apply rules for impossible values

Now decide how to handle:

- Negative prices,
- Very large prices that are implausible,
- Sentinel codes (like `-999`).

> **Tasks:**
>
> 1. Use `dplyr::filter()` and summaries (`summary()`, `quantile()`) to identify at least **two kinds** of “bad” price values.


```{r}
# Your code here

```

> 2. Write down your **rules** for how to handle each type (e.g., set to `NA`, drop rows, cap values).
> 3. Implement those rules in a new variable (e.g., `PRICE_final`) using `dplyr::mutate()` and `dplyr::case_when()`.

```{r}
# Your code here

```

> 4. Briefly describe your rules in text and justify them using the **validity** and **completeness** dimensions of data quality.


# Cleaning Categorical & String Fields (30–40 minutes)

Now work on **categorical** and **string** variables, such as:

- `DEVICE_GEO_REGION`
- `DEVICE_GEO_ZIP`
- `RESPONSE_TIME`

Remember: **this dataset only includes bids from Oregon**, so a correct region should be coded as `"OR"`. Other values in `DEVICE_GEO_REGION` represent **modified/mangled versions** of `"OR"`.

Helpful string tools:

- `stringr::str_trim()`, `stringr::str_to_upper()`, `stringr::str_replace()`
- `stringr::str_detect()`, `stringr::str_pad()`, `stringr::str_length()`
- `dplyr::count()` to see category frequencies

## DEVICE_GEO_REGION: standardizing Oregon codes

> **Tasks:**
>
> 1. Use `dplyr::count(DEVICE_GEO_REGION, sort = TRUE)` to list distinct region values and their frequencies.

```{r}
# Your code here

```

> 2. Identify **all values** that clearly correspond to **Oregon**, just coded inconsistently (e.g., extra punctuation, different case, misspellings).


```{r}
# Your code here

```

> 3. Define a **standard representation** (`"OR"`) and create a cleaned version (e.g., `DEVICE_GEO_REGION_clean`) where:
>    - All Oregon variants are standardized to `"OR"`.
>    - Other values (if any) are treated according to a rule you decide (e.g., set to `NA` if they cannot be interpreted as Oregon).


```{r}
# Your code here

```

> 4. In **2–3 sentences**, explain:
>    - How you detected inconsistent codes,
>    - The **set-membership rule** you are enforcing (valid region codes for this dataset).

## DEVICE_GEO_ZIP: formats and sentinels

Treat ZIP codes as **strings**, not numbers.

> **Tasks:**
>
> 1. Convert ZIP codes to character and use `dplyr::count()` to inspect common values.


```{r}
# Your code here

```

> 2. Identify suspicious ZIP values:
>    - Too short / too long,
>    - Obvious sentinel codes (e.g., `-999`, `9999`),
>    - Non-digit characters.

```{r}
# Your code here

```

> 3. Decide on a **pattern** for valid ZIP codes (e.g., Oregon 5-digit ZIPs).
>    - Consider using `stringr::str_length()`, `stringr::str_detect()` with a regular expression pattern, and `stringr::str_pad()` if necessary.

```{r}
# Your code here

```

> 4. Create a cleaned version (e.g., `DEVICE_GEO_ZIP_clean`) that:
>    - Trims whitespace,
>    - Converts sentinel codes to `NA`,
>    - Enforces your chosen ZIP pattern.

```{r}
# Your code here

```

> 5. In a short paragraph, describe your rule and why it is reasonable given the context (Oregon-only data).


## RESPONSE_TIME: prefixes and extra characters

The `RESPONSE_TIME` variable may include:

- A **text prefix** (e.g., “RESPONSE_TIME:” or typo’d variants),
- Extra characters at the end (e.g., punctuation),
- Numeric content representing a duration.

> **Tasks:**
>
> 1. Use a small `sample()` of `RESPONSE_TIME` to inspect the raw patterns.

```{r}
# Your code here

```

> 2. Write a rule for how to:
>    - Remove any “RESPONSE_TIME:” prefixes (including likely typos),
>    - Strip extra non-numeric characters from the end,
>    - Convert the remaining content to a numeric type.
> 3. Implement your rule in a cleaned variable (e.g., `RESPONSE_TIME_clean`) using `stringr::str_replace()` or related functions and `as.numeric()`.

```{r}
# Your code here

```

> 4. Count how many `NA`s you get in `RESPONSE_TIME_clean` and briefly interpret what those `NA`s mean.

```{r}
# Your code here

```

> 5. Connect this to the idea of **regular-expression patterns** as a tool for enforcing validity.

# Date–Time Cleaning: TIMESTAMP (20–30 minutes)

The `TIMESTAMP` field records when a bid occurred, but the **format is inconsistent** (for example, some dates may use `-` and others `/`).

Useful tools:

- `as.character()`
- `stringr::str_detect()`
- `lubridate::parse_date_time()`, `lubridate::ymd_hms()`, `lubridate::mdy_hms()`

## Inspect TIMESTAMP patterns

> **Tasks:**
>
> 1. Look at a small random sample of `TIMESTAMP` values.

```{r}
sample(bids$TIMESTAMP)
```

> 2. Use `stringr::str_detect()` to count how many timestamps use `"-"` versus `"/"` in the date.

```{r}
stringr::str_detect(bids$TIMESTAMP, "-") %>%
  sum()
stringr::str_detect(bids$TIMESTAMP, "/") %>%
  sum()
```

> 3. Summarize in 1–2 sentences what you observe.

There are two obvious issues so far: NA values in the date portion of the variable, and some entries are listed with "/" rather than "-". There are 1157 entries where "/" is used.

## Parse TIMESTAMP into a POSIXct variable

> **Tasks:**
>
> 1. Use `lubridate::parse_date_time()` (or similar) to create `TIMESTAMP_clean`:
>    - Allow for multiple possible formats (e.g., one order for `ymd HMS` and one for `mdy HMS` to see examples scroll paste `?lubridate::parse_date_time()` in your console and scroll all the way to the bottom of the function's help tab),
>    - Specify a time zone (e.g., `"UTC"`).

```{r TIMESTAMP Parse}
TIMESTAMP_clean <- lubridate::parse_date_time(bids$TIMESTAMP, c("ymd HMS", "mdy HMS"), tz = "UTC")

```

> 2. Count how many `NA`s result from parsing.

```{r TIMESTAMP NAs}
sum(is.na(TIMESTAMP_clean))
```

> 3. Use `summary()` or `range()` to examine the minimum and maximum of `TIMESTAMP_clean`.

```{r TIMESTAMP summary}
summary(TIMESTAMP_clean)
```

> 4. Briefly comment on:
>    - Whether the time range seems reasonable,
>    - What kinds of entries failed to parse (if any).

The shown range for the TIMESTAMP variable seems reasonable, given that we only have about a day's worth of data. There were entries that had a given time, but the date was labeled as NA. These seem to be the entries that did not parse into the POSIXct variable. Though the date is NA, we know the range is within a day. There may be possibility to recover those date values by referencing the given time and assigning the date associated with it.

# Keys, Duplicates, and Out-of-Range Values (20–30 minutes)

## Candidate keys and duplicates

Using your earlier reasoning:

> **Tasks:**
>
> 1. Choose one variable or a combination of variables that you **believe should uniquely identify** a row (a key).

- According to the data dictionary, the grain should be `(AUCTION_ID, bidder/seat, size, …)`.
- We have not been given `bidder/seat`, `bidder`, or `seat`.
- We must find a different unique key or combination of variables to make a **composite key**.
- Based on the available variables, a composite key *may* be `AUCTION_ID`, `SIZE`, and `PRICE`.


> 2. Use `dplyr::count()` to check whether that key is truly unique or if there are duplicates.

```{r candidate-key-check}
# Candidate composite key: AUCTION_ID + SIZE + PRICE

# Compare rows vs. distinct by composite key
bids_raw %>%
  dplyr::summarise(
    n_rows          = dplyr::n(),
    n_unique_auct   = dplyr::n_distinct(AUCTION_ID)#, SIZE, PRICE)
  )

# Show composite key rows that appear more than once
candidate_key_counts <- bids_raw %>%
  dplyr::count(AUCTION_ID, SIZE, PRICE, sort = TRUE)

candidate_key_counts %>%
  dplyr::filter(n > 1) %>%
  head(20)
```

- We see that with the three variables `AUCTION_ID`, `SIZE`, and `PRICE`, we do not have a composite key to uniquely identify rows.
- This could be party due to exact full-row duplicates (inspected next).


> 3. Check for **full-row duplicates** using `dplyr::distinct()` or by counting occurrences across all columns.

```{r full-row-duplicate-check}
# Total number of rows in dataset
n_rows <- nrow(bids_raw)

# Number of distinct rows across all columns
n_rows_distinct <- bids_raw %>%
  dplyr::distinct() %>%
  nrow()

# Number of exact duplicate rows
n_exact_duplicates <- n_rows - n_rows_distinct

cat("Total rows:                      ", n_rows, "\n")
cat("Distinct rows (all columns):      ", n_rows_distinct, "\n")
cat("Exact duplicate rows (by count):  ", n_exact_duplicates, "\n")
```

- We see that we have 2,434 rows which are exact duplicates.


> 4. Decide on a rule for handling duplicates:
>    - Drop exact duplicates?
>    - Keep first occurrence and drop the rest?
>    - Something else?

- We will drop full duplicate rows, keeping a single row in each set of duplicates.


> 5. Implement your rule and create a de-duplicated dataset (e.g., `bids_nodup`).

```{r create-bids-nodup}
# Remove exact full-row duplicates (keep one copy)
bids_nodup <- bids_raw %>%
  distinct()

cat("Rows in original dataset:   ", nrow(bids_raw), "\n")
cat("Rows in bids_nodup dataset: ", nrow(bids_nodup), "\n")
```

- We now have a count of **441,535 rows**, which matches the number of distinct rows in the raw data.

We can now re-check our initial candidate for a composite key:

```{r candidate-key-recheck}
# Candidate composite key: AUCTION_ID + SIZE + PRICE

# Compare rows vs. distinct by composite key
bids_nodup %>%
  dplyr::summarise(
    n_rows         = dplyr::n(),
    n_unique_key   = dplyr::n_distinct(AUCTION_ID, SIZE, PRICE)
  )

# Show composite key rows that appear more than once
candidate_key_counts <- bids_nodup %>%
  dplyr::count(AUCTION_ID, SIZE, PRICE, sort = TRUE)

candidate_key_counts %>%
  dplyr::filter(n > 1) %>%
  head(20)
```

- Although we have removed full duplicate rows, this particular set of variables still does not work as a composite key for unique row identification.

We can create a helper function to allow us to search available variables for a combination which will work as a key. To avoid computational blowout, we focus only on variables which logically make the most sense and limit our number of variables per composite key to 4:

```{r composite-key-helper}
check_candidate_keys <- function(data, cols, max_size = 4, show_full = TRUE) {
  results <- list()
  idx <- 1L
  
  for (k in seq_len(max_size)) {
    combos <- combn(cols, k, simplify = FALSE)
    
    for (combo in combos) {
      n_unique <- data %>%
        distinct(across(all_of(combo))) %>%
        nrow()
      
      results[[idx]] <- tibble(
        combo   = paste(combo, collapse = " + "),
        size    = k,
        n_rows  = nrow(data),
        n_unique = n_unique,
        is_key  = (n_unique == nrow(data))
      )
      idx <- idx + 1L
    }
  }
  
  results <- bind_rows(results)
  
  if (show_full) {
    # Full table (shown if needed for further exploration)
    results %>%
      arrange(desc(is_key), size, desc(n_unique))
  } else {
    # Compact summary
    results %>%
      group_by(size) %>%
      summarise(
        num_combos   = n(),
        num_keys     = sum(is_key),
        max_n_unique = max(n_unique),
        .groups = "drop"
      ) %>%
      arrange(size)
  }
}
```


We then use this function to check the most likely candidate variables:

```{r composite-key-search}
composite_candidates <- c(
  "AUCTION_ID",
  "SIZE",
  "PRICE",
  "RESPONSE_TIME",
  "BID_WON",
  "PUBLISHER_ID",
  "DEVICE_TYPE",
  "DEVICE_GEO_CITY",
  "DEVICE_GEO_ZIP",
  "DEVICE_GEO_LAT",
  "DEVICE_GEO_LONG",
  "TIMESTAMP"
)

key_results <- check_candidate_keys(
  data     = bids_nodup,
  cols     = composite_candidates,
  max_size = 4,
  show_full = FALSE
)

key_results
```

- From these results, we can see that no combination (up to four variables) of these candidate variables will create a composite key to uniquely identify the rows of this dataset. The **maximum number of unique rows in any combination was 441,172**, but our dataset with no full-row duplicates is 441,535.


> 6. In 2–3 sentences, explain your choice and connect it to the **uniqueness** dimension of data quality.

Our main design choice for improving the **uniqueness** dimension was to remove exact full-row duplicates, keeping one copy of each identical record to avoid double-counting while preserving all genuinely different rows. 
After removing these duplicates, we searched for single-column and composite keys (up to four candidate fields) and found that none of our tested combinations uniquely identifies every row. The data dictionary’s grain definition depends on missing bidder/seat fields.
Thus, we can enforce uniqueness only by eliminating exact duplicates, not by defining a true primary key from the available columns.


## Out-of-range latitude/longitude (optional but recommended)

Look at `DEVICE_GEO_LAT` and `DEVICE_GEO_LONG`.

> **Tasks:**
>
> 1. Use `summary()` or `dplyr::summarise()` to get the minimum and maximum latitude and longitude.
>
> 2. Compare the ranges to what you know about **Oregon’s** geographic coordinates (you can approximate based on general US geography; no need for exact boundaries).
>
> 3. Decide which coordinates are clearly **implausible**.

Using Oregon’s approximate bounds (according to "MapsofTheWorld.com"):

- Latitude: **$42^\circ$ N to $46.15^\circ$ N**  
- Longitude: **$-124.3^\circ$ W to $-116.45^\circ$ W**

(We allow a small buffer around these ranges to avoid accidentally marking
borderline-but-valid values as invalid.)

```{r lat-long-summary, message=FALSE}
# Added slight buffers to values
latlong_summary <- bids_nodup %>%
  summarise(
    n_rows          = n(),
    min_lat         = min(DEVICE_GEO_LAT, na.rm = TRUE),
    max_lat         = max(DEVICE_GEO_LAT, na.rm = TRUE),
    n_lat_outside   = sum(DEVICE_GEO_LAT < 41    | DEVICE_GEO_LAT > 47,     na.rm = TRUE),
    min_long        = min(DEVICE_GEO_LONG, na.rm = TRUE),
    max_long        = max(DEVICE_GEO_LONG, na.rm = TRUE),
    n_long_outside  = sum(DEVICE_GEO_LONG < -125 | DEVICE_GEO_LONG > -116, na.rm = TRUE),
    n_long_10_over  = sum(DEVICE_GEO_LONG < -121 & DEVICE_GEO_LONG > -127.5, na.rm = TRUE)
  )

latlong_summary %>%
  kable(caption = "Lat/long values compared to approximate Oregon bounds") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```

  - Exactly 100 of rows have **longitudes** outside Oregon’s expected range (exact 100 implies an added "bomb"). 
  - We also see that the majority of the points lie about 10 degrees over from these shifted values (Final output column: tested by checking number of points in small range about 10 degrees away). This implies an exact shift of 10 for these 100 points. 
  
To justify correcting these 100 shifted longitudinal values, we compare suspicious rows ("bad longitudes") to the valid ones ("good longitudes") using `DEVICE_GEO_ZIP` and `DEVICE_GEO_CITY`

```{r id-shifted-rows}
# ID the "bad longitude" rows
bad_long <- bids_nodup %>%
  filter(DEVICE_GEO_LONG < -125 | DEVICE_GEO_LONG > -116)

# ID the "good longitude" rows
good_long <- bids_nodup %>%
  filter(DEVICE_GEO_LONG >= -125 & DEVICE_GEO_LONG <= -116)

# Verify count of "bad longitude" rows is 100
nrow(bad_long)

```
```{r check-shift}
# Verify shift will put "bad longitude" rows in bounds
bad_long_shift_check <- bad_long %>%
  mutate(long_shifted = DEVICE_GEO_LONG + 10) %>%
  summarise(
    n_rows                = n(),
    min_long_orig         = min(DEVICE_GEO_LONG, na.rm = TRUE),
    max_long_orig         = max(DEVICE_GEO_LONG, na.rm = TRUE),
    min_long_shifted      = min(long_shifted, na.rm = TRUE),
    max_long_shifted      = max(long_shifted, na.rm = TRUE),
    n_shifted_in_bounds   = sum(long_shifted >= -125 & long_shifted <= -116, na.rm = TRUE)
  )

bad_long_shift_check
```
We see that *all shifted values* fall within the Oregon range for longitude. 

To verify via **Zip** and **City**, we first check to see how many of the 100 have `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP` data available:

```{r bad-long-na-check}
bad_long %>%
  summarise(
    n_bad         = n(),
    n_zip_city_ok = sum(!is.na(DEVICE_GEO_ZIP) & !is.na(DEVICE_GEO_CITY)),
    n_zip_na      = sum(is.na(DEVICE_GEO_ZIP)),
    n_city_na     = sum(is.na(DEVICE_GEO_CITY))
  )
```

We see that **98 of the 100** rows have `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP` data available. We first address those rows.

```{r zip-city-shift-check}
# Collapse "good" longitudes to one reference value per ZIP + CITY
good_zipcity <- good_long %>%
  filter(!is.na(DEVICE_GEO_ZIP), !is.na(DEVICE_GEO_CITY)) %>%
  group_by(DEVICE_GEO_ZIP, DEVICE_GEO_CITY) %>%
  summarise(
    ref_long = mean(DEVICE_GEO_LONG, na.rm = TRUE),
    .groups  = "drop"
  )

# Join bad rows (w/ valid ZIP + CITY) to these reference values
paired_zipcity <- bad_long %>%
  filter(!is.na(DEVICE_GEO_ZIP), !is.na(DEVICE_GEO_CITY)) %>%
  left_join(
    good_zipcity,
    by = c("DEVICE_GEO_ZIP", "DEVICE_GEO_CITY")
  ) %>%
  mutate(diff_long = ref_long - DEVICE_GEO_LONG)

# Summarise differences
paired_zipcity %>%
  summarise(
    n_bad_with_match = sum(!is.na(ref_long)),
    min_diff         = min(diff_long, na.rm = TRUE),
    max_diff         = max(diff_long, na.rm = TRUE),
    mean_diff        = mean(diff_long, na.rm = TRUE),
    n_near_10        = sum(abs(diff_long - 10) < 0.1, na.rm = TRUE)
  )
```

These results **justify the 10 degree shift** for these 98 values.

We now address the 2 values which `NA` for `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP`. We compare these to other rows with the same `AUCTION_ID`:

```{r auction-shift-check-clean, message=FALSE}
# Identify bad rows with missing ZIP/CITY
bad_missing_loc <- bad_long %>%
  filter(is.na(DEVICE_GEO_ZIP) | is.na(DEVICE_GEO_CITY))

# Reference longitudes for each auction (from good rows)
good_auction <- good_long %>%
  group_by(AUCTION_ID) %>%
  summarise(ref_long_auction = mean(DEVICE_GEO_LONG, na.rm = TRUE),
            .groups = "drop")

# Join & compute shift difference
auction_shift_check <- bad_missing_loc %>%
  left_join(good_auction, by = "AUCTION_ID") %>%
  mutate(diff_long = ref_long_auction - DEVICE_GEO_LONG) %>%
  select(
    AUCTION_ID,
    DEVICE_GEO_LAT,
    DEVICE_GEO_LONG,
    ref_long_auction,
    diff_long
  )

auction_shift_check %>%
  kable(caption = "Two NA-location rows with auction-based longitude comparison") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```

These results show these two rows are **shifted exactly 10 degrees** from the other longitude values of rows with the same respective `AUCTION_ID`, justifying a shift for these rows as well.


> 4. Define rules to create cleaned versions (e.g., `DEVICE_GEO_LAT_clean`, `DEVICE_GEO_LONG_clean`) where:
>    - Plausible values are kept,
>    - Implausible values are set to `NA` or corresponding rows are removed.

- All `DEVICE_GEO_LAT` values are within plausible Oregon range.
    - Transfer as-is to `DEVICE_GEO_LAT_clean`.
- There are **100 rows** with `DEVICE_GEO_LONG` outside of a plausible Oregon range.
    - 98 of these 100 have rows with matching `DEVICE_GEO_CITY` and `DEVICE_GEO_ZIP` which show an exact 10 degree shift in longitude.
    - Remaining 2 rows show all matching `AUCTION_ID`s have exact 10 degree shift in longitudinal values.
    - These justify a **10 degree shift for all 100 out-of-bounds longitudinal values** for `DEVICE_GEO_LONG_clean`.

```{r lat-long-clean-final}
bids_nodup <- bids_nodup %>%
  mutate(
    # Latitude: requires no correction
    DEVICE_GEO_LAT_clean = DEVICE_GEO_LAT,
    
    # Longitude: correct the 100 shifted values
    DEVICE_GEO_LONG_clean = case_when(
      DEVICE_GEO_LONG < -125 | DEVICE_GEO_LONG > -116 ~ DEVICE_GEO_LONG + 10,
      TRUE ~ DEVICE_GEO_LONG
    )
  )

# Verify all rows now have valid Oregon longitude values
bids_nodup %>%
  summarise(
    n_rows = n(),
    n_long_outside_clean =
      sum(DEVICE_GEO_LONG_clean < -125 | DEVICE_GEO_LONG_clean > -116)
  )
```

> 5. Document the rules you used and why.

- All **latitude values** were already within Oregon’s expected range ($42^\circ$ N to $46.15^\circ$ N), so they were **kept as-is**.
- Exactly **100 longitude values were shifted by $+10^\circ$** based on ZIP/city matching and auction-level comparisons. Correcting these restored them to plausible Oregon coordinates ($-124.3^\circ$ W to $-116.45^\circ$ W).

# Creating a Final Cleaned Dataset (10–15 minutes)

Now pull everything together.

> **Tasks:**
>
> 1. Create a final cleaned dataset (e.g., `bids_clean`) that:
>    - Uses your cleaned variables (`PRICE_final`, `DEVICE_GEO_REGION_clean`, `DEVICE_GEO_ZIP_clean`, `RESPONSE_TIME_clean`, `TIMESTAMP_clean`, cleaned lat/long, etc.),
>    - Drops unnecessary intermediate columns (e.g., `_raw`, helper strings) that you don’t need for analysis.

```{r}
# Your code here

```

> 2. Check the structure of `bids_clean` with `dplyr::glimpse()`.

```{r}
# Your code here

```

> 3. Optionally, write the cleaned dataset to disk (for your own future use) using something like `save()` (see `?save` for its use and an example) to save in `.RData` format **but do not commit the cleaned data file** to the remote repository.

```{r}
# Your code here (optional)

```

> 4. In a short paragraph, describe what your final cleaned dataset now represents and what major issues have been addressed.

# Data Cleaning Log & Reflection (10–15 minutes)

## Data cleaning log (table)

Create a succinct table summarizing the main problems and fixes.

Use the template below and fill it in with your team’s work. **One row (PRICE) is completed as an example; you should complete the remaining rows yourselves.**

| Variable | Problem Detected (brief) | Data Quality Dimension(s) | Cleaning Action (brief) |
|----------|-------------------------|---------------------------|--------------------------|
| <FULL DATASET> | full-row duplicates | uniqueness | removed duplicates while maintaining single copy of each identical row |
| PRICE | Includes non-numeric characters | Validity | Extract numeric portion and convert to numeric |
| DEVICE_GEO_REGION | *(team describes)* | *(team selects)* | *(team describes)* |
| DEVICE_GEO_ZIP | *(team describes)* | *(team selects)* | *(team describes)* |
| DEVICE_GEO_LAT | NONE: All values verified to be within plausible Oregon range | Validity, Accuracy | Copied into DEVICE_GEO_LAT_clean as-is |
| DEVICE_GEO_LONG | 100 values outside Oregon plausible range | Validity, Accuracy | Shifted 100 values by 10 degrees (justifications above) |
| RESPONSE_TIME | *(team describes)* | *(team selects)* | *(team describes)* |
| TIMESTAMP | *(team describes)* | *(team selects)* | *(team describes)* |
| Other variable(s) | *(optional)* | *(optional)* | *(optional)* |

## Reflection (5–8 sentences)

Write a short reflection addressing:

- Which data cleaning tasks were the most **time-consuming** or **surprising**?
- Which functions or workflows (e.g., `mutate()`, `case_when()`, `str_*()`, `parse_date_time()`, `distinct()`) were most valuable?
- One thing you will do **differently** the next time you receive a new messy dataset.
- Whether/how you used generative AI for this lab and how it affected your understanding.
