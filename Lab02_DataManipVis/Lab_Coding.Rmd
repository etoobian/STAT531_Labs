---
title: "Lab02: Data Manipulation and Visualization with R"
author: "TECK"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)

library(dplyr)
library(ggplot2)
library(arrow)

```

# Overview

In this lab, your team will explore the online advertising auctions data.
Your goals are to:

-   Practice **data manipulation** in R (using tidyverse).
-   Practice **data visualization** using concepts from the Data Visualization session.
-   Identify **3–4 data quality issues** that would matter in a real analysis workflow.
-   Communicate results clearly and professionally as a team.

This lab is designed to take **2–3 hours** of focused collaborative work.

> Today, you are only exploring, visualizing, interpreting, and identifying suspicious looking issues.

------------------------------------------------------------------------

# Git Workflow, Collab, and Use of Generative AI

This is a **team-based exploratory analysis**.

## Git Workflow

-   **One team member** should fork the course lab repository.
-   In the team’s cloned fork, create the `data/` folder:
-   That person should **add the rest of the team as collaborators**.
-   All team members should clone the **same shared fork** (creating an RStudio project).

To keep collaboration smooth:

-   Pull before you start working.
-   Commit frequently, with clear messages (e.g., "added price histogram", "investigated region codes").
-   Remember to pull before pushing.
-   Push your work regularly.

I will review the **Git commit history** to understand who contributed what.
Individual grades depend on **meaningful contribution** from each student.

## Team Planning (Required)

Before you begin, read the entire lab together (5–10 minutes).

-   Make sure everyone understands the goals, deliverables, and sections.

Decide how to divide the work (3–4 members).

-   As a team, choose a coordinator for each major section, and within sections split the work

    * Section 1: Initial Orientation
    * Section 2: Data Exploration (dplyr)
    * Section 3: Visualizations
    * Section 4: Synthesis & Reflection
    
-   Section coordinators will ensure their sections are completed throughout the lab's development

Create a Jira Kanban Board (team facilitator).

-   Go to Jira → Create Project → Kanban template

-   Create a board titled: “Ad Bids Lab – Team <YourTeamName>”

-   Add cards for each task/subtask, including:

    * Task name
    * Description
    * Assigned team member
    * Due date (based on team timeline)
    
Upload a screenshot (I'll check the time and date of the file) of the Kanban board.

-   A screenshot showing all initial cards in the To Do column and assignments.

-   Save it in your repository under: `planning/kanban_screenshot.png`

-   Add it **here**.

Make use of the team charter that you designed in Lab01, using the Kanban board throughout the project to keep you on task, correcting issues that arise and readjusting as needed.


## Use of Generative AI

You **may** use Gen.AI tools **as support**, not as a replacement.

Appropriate AI uses:

-   Understanding R errors
-   Asking for explanations of functions & syntax
-   Debugging when stuck
-   Checking whether your code is reasonable

Inappropriate uses:

-   Asking AI to complete most or all of the lab
-   Copying AI-generated code you don’t understand
-   Using AI to write your interpretations or summaries

Your code and reasoning must reflect **your own team’s understanding**.

------------------------------------------------------------------------


# Data and Setup

1.  Each team member should download from Canvas two files in `Modules → ProjectInformation`:

-   `bids_data_vDTR.parquet`

-   A **data dictionary** describing each variable

3.  In your local repo add the data and data dictionary to the `data` folder

4.  Add the data and dictionary file to your .gitignore

5.  Verify that Git is ignoring the files (in the RStudio terminal write `git status` these files should not be listed in the output)

6.  Load `tidyverse`, `ggplot2` and any other packages you find useful (`arrow`, `lubridate`, etc.)

```{r load-data}
# Load packages
library(tidyverse)
library(arrow)

# Load main data set from the repo's data/ folder
bids <- read_parquet("../data/bids_data_vDTR.parquet")

# Work with a copy called df
df <- bids

# Quick preview of the data
glimpse(df)

```

------------------------------------------------------------------------

# 1. Initial Orientation (10–15 minutes)

Answer the following in complete sentences. Use code chunks to support your answers, but write the answers in prose.

1.  How many rows and columns are in the dataset?
2.  According to the data dictionary, what does each row represent?
3.  Which variable(s) are the `key(s)` (i.e., the row identifiers)
3.  Which variables appear:
    -   numerical?
    -   categorical?
    -   suspicious or inconsistent with the dictionary?
4.  Name **two variables** you expect to matter most to advertisers and explain why.

*(Write your answers below.)*

------------------------------------------------------------------------

# 2. Data Exploration Using `dplyr` (30–40 minutes)

Use functions you are comfortable with from `dplyr` and `tidyr`.
Tables and summaries should be created in code chunks; explanations should be written as text.

## 2.1 Geographic Exploration

Explore the geographical fields (such as `DEVICE_GEO_COUNTRY`, `DEVICE_GEO_REGION`, `DEVICE_GEO_CITY`, `DEVICE_GEO_ZIP`).

Tasks:

1.  Create tables with the **top 3 regions** and the **top 10 cities** by count of rows.
2.  Compare the region codes to what the data dictionary says they should look like.
3.  Identify **at least one** region code that is clearly suspicious, and explain why.

```{r geo-explore}
# TODO: your code for geographic exploration
```

*(Write your interpretation below.)*

------------------------------------------------------------------------

## 2.2 Price Behavior

Study the `PRICE` variable.

Tasks:

1.  Produce numerical summaries of `PRICE` (e.g., min, max, median, selected quantiles).
2.  Identify implausible price ranges or values and explain why they are implausible.
3.  Investigate whether suspicious prices seem to cluster by **one** of the following:
    -   publisher,
    -   region, **or**
    -   ZIP code.

Choose only one dimension and describe what you find.

```{r price-explore}
library(dplyr)

# 1) Create a cleaned numeric PRICE variable
df_price <- df %>%
  mutate(
    PRICE_num = as.numeric(PRICE),
    PRICE_num = na_if(PRICE_num, -999),   # turn -999 into NA
    PRICE_num = na_if(PRICE_num, 0)       # turn 0 into NA
  )

# 2) Numerical summary of cleaned PRICE
price_summary <- df_price %>%
  summarize(
    n_non_missing   = sum(!is.na(PRICE_num)),
    n_zero_or_neg   = sum(PRICE_num <= 0, na.rm = TRUE),
    n_very_small    = sum(PRICE_num > 0 & PRICE_num < 0.001, na.rm = TRUE),
    Min             = min(PRICE_num, na.rm = TRUE),
    Q1              = quantile(PRICE_num, 0.25, na.rm = TRUE),
    Median          = median(PRICE_num, na.rm = TRUE),
    Mean            = mean(PRICE_num, na.rm = TRUE),
    Q3              = quantile(PRICE_num, 0.75, na.rm = TRUE),
    Max             = max(PRICE_num, na.rm = TRUE)
  )

price_summary

# 3) Price by ZIP code (to see if strange prices cluster by ZIP)
zip_price <- df_price %>%
  filter(
    !is.na(PRICE_num),
    !is.na(DEVICE_GEO_ZIP),
    nchar(DEVICE_GEO_ZIP) == 5
  ) %>%
  group_by(DEVICE_GEO_ZIP) %>%
  summarize(
    n_obs        = n(),
    median_price = median(PRICE_num),
    mean_price   = mean(PRICE_num),
    max_price    = max(PRICE_num)
  ) %>%
  arrange(desc(median_price))

# Show top 10 ZIP codes by median price
head(zip_price, 10)

```

*(Write your interpretation below.)*
With a mean near 0.48 and a median of about 0.20, our numerical summaries demonstrate that the majority of bid prices are modest and positive.  Nevertheless, a non-trivial number of implausible values, including zeros, negative numbers, and incredibly low prices below 0.001, are also observed.  We treated 0, −999, and extremely low prices as problematic because these values are probably data entry or coding errors rather than actual auction prices.

Suspiciously high prices tend to concentrate in a small number of ZIP codes when prices are grouped by ZIP code.  The median and maximum prices of a small subset of ZIP codes are significantly higher than those of the entire dataset.  This implies that certain areas might have systematic problems with data quality (such as inaccurate units or misrecorded bids), and these ZIP codes ought to be thoroughly examined in subsequent cleaning or modeling stages.
------------------------------------------------------------------------

## 2.3 Response Time Behavior

Investigate the `RESPONSE_TIME` variable.

Tasks:

1.  Examine its data type and typical values.
2.  Look for common patterns or prefixes/suffixes in the values.
3.  Identify an issue that would prevent straightforward numerical analysis.

```{r rt-explore}
# TODO: your code for exploring RESPONSE_TIME
```

*(Write your interpretation below.)*

------------------------------------------------------------------------

# 3. Visualization Challenges (45–60 minutes)

Create **three** high-quality visualizations using `ggplot2`.

Each figure must include:

-   A clear and professional title
-   Labeled axes
-   A readable theme (e.g., `theme_bw()`, `theme_minimal()`)
-   Thoughtful design choices (color, faceting, ordering, etc.)
-   A **3–5 sentence interpretation**

## 3.1 Plot 1 — Distribution of Bid Prices

Create a visualization that reveals:

-   The overall shape of the price distribution
-   Outliers or impossible values
-   Any issues you suspect based on the data dictionary

```{r plot-price}
library(ggplot2)
library(dplyr)

# Start with cleaned numeric price column
df_price <- df %>%
  mutate(
    PRICE_num = suppressWarnings(as.numeric(PRICE)),
    PRICE_num = ifelse(PRICE_num < 0 | PRICE_num > 200, NA, PRICE_num) # optional cleaning
  )

# Plot histogram
ggplot(df_price, aes(x = PRICE_num)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Distribution of Cleaned Bid Prices",
    x = "PRICE (numeric)",
    y = "Count"
  ) +
  theme_minimal()

```

*Interpretation (3–5 sentences):*
The majority of bids are concentrated at extremely low price points, and the distribution of cleaned bid prices is strongly skewed to the right.  A few exceptionally high prices show up as obvious outliers, which could be the result of incorrect data entry or the use of different units.  Extremely high values may raise suspicions because the data dictionary states that prices should typically fall within a reasonable range.  Overall, the distribution has a long tail and a lot of small bids, which is normal for online auction markets but still needs to be cleaned up.
------------------------------------------------------------------------

## 3.2 Plot 2 — Price by Zip City

Compare a central price measure (mean or median) across cities

Guidance:

-   Be explicit about whether you use mean or median.
-   Consider sorting cities by the measure you plot.
-   Highlight any cities whose price behavior appears inconsistent or suspicious.

```{r plot-price-region}
library(dplyr)
library(ggplot2)

# Use the cleaned PRICE_num we created before
# Summarize price by city
city_price <- df_price %>%
  filter(
    !is.na(PRICE_num),
    !is.na(DEVICE_GEO_CITY)
  ) %>%
  group_by(DEVICE_GEO_CITY) %>%
  summarize(
    n_obs        = n(),
    median_price = median(PRICE_num),
    mean_price   = mean(PRICE_num)
  ) %>%
  # keep only cities with enough data (e.g., at least 50 bids)
  filter(n_obs >= 50) %>%
  arrange(desc(median_price)) %>%
  slice_head(n = 15)   # top 15 cities by median price

# Bar plot of median price by city
ggplot(city_price,
       aes(x = reorder(DEVICE_GEO_CITY, median_price),
           y = median_price)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Median cleaned bid price by city (top 15)",
    x = "City",
    y = "Median cleaned PRICE"
  ) +
  theme_minimal()

```

*Interpretation (3–5 sentences):*
The top cities are displayed in the plot according to the median cleaned bid price.  A few cities stand out with noticeably higher median prices, but the majority of cities have comparatively similar median prices.  These more expensive cities might represent truly more valuable ad markets, but they might also point to issues with the quality of the data (such as different units or incorrectly coded bids).  These patterns are based on a sizable number of observations because we only included cities with at least 50 bids.  It would be crucial to determine whether these expensive cities also exhibit other questionable trends in the data in subsequent analysis.

------------------------------------------------------------------------

## 3.3 Plot 3 — Analyst’s Choice

Choose a visualization that you believe would be useful to a data science team at an advertising company.

Possible ideas:

-   Auction volume by hour of day
-   Differences between winning and losing bids
-   ZIP code irregularities
-   Response time patterns
-   Publisher-level differences

Your plot should answer a clear question, and your interpretation should explain what you learned.

```{r plot-analyst-choice}

library(dplyr)
library(ggplot2)

# Create cleaned numeric price column
df_price <- df %>%
  mutate(
    PRICE_num = suppressWarnings(as.numeric(PRICE)),
    PRICE_num = ifelse(PRICE_num < 0 | PRICE_num > 200, NA, PRICE_num)
  )

# Summarize price by device type
device_price <- df_price %>%
  filter(!is.na(PRICE_num), !is.na(DEVICE_TYPE)) %>%
  group_by(DEVICE_TYPE) %>%
  summarize(
    n_obs        = n(),
    median_price = median(PRICE_num),
    mean_price   = mean(PRICE_num)
  ) %>%
  arrange(desc(median_price))

# Plot median price by device type
ggplot(device_price,
       aes(x = reorder(DEVICE_TYPE, median_price),
           y = median_price)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Median Bid Price by Device Type",
    x = "Device Type",
    y = "Median Price"
  ) +
  theme_minimal()

```

*Interpretation (3–5 sentences):*
The median bid price for various device types is displayed in this graphic.  Stronger advertiser demand or higher-value impressions on particular platforms are indicated by the significantly higher median prices of some device categories.  In the meantime, a number of device types exhibit extremely low median values, which may indicate different auction dynamics or lower-quality traffic.  These variations suggest that device type should be taken into account in downstream modeling since it has a significant impact on auction pricing.

------------------------------------------------------------------------

# 4. Synthesis & Reporting (20–25 minutes)

## 4.1 Data Quality Issues

Based on your work above, list **3–4** data quality issues you discovered.
For each issue:

-   Name the affected variable(s).
-   Describe what the issue is.
-   Provide evidence (e.g., from a summary, table, or plot).
-   Suggest one way you would address it in a later **data cleaning** lab.

*(Write your answer here in paragraphs or bullet points.)*

------------------------------------------------------------------------

## 4.2 Implications for Analytics

In 1–2 paragraphs, discuss how the issues you identified could affect:

-   Business decisions,
-   Reporting to advertisers, and/or
-   Predictive modeling.

Which issues are most critical to fix before building models, and why?

*(Write your answer here.)*
Downstream analytics are significantly impacted by the data quality problems found in PRICE, including zeros, negative values, -999 codes, and incredibly small, unrealistic prices.  These values would skew model estimates, skew summary statistics, and produce deceptive patterns if they were not adjusted, particularly in tasks like campaign performance evaluation, price prediction, and optimization.

Inconsistencies at the ZIP and city levels may also indicate that some areas have different units of measurement or systematic issues with data entry.  This implies that if these problems are not resolved, spatial or regional models may yield unreliable results.  In general, validating price ranges, eliminating impossible values, and guaranteeing consistency across geographic fields are the most important tasks prior to modeling.  Resolving these problems enhances the validity of any analytical conclusions drawn from the dataset.
------------------------------------------------------------------------

## 4.3 Reflection: Tools, Workflow, AI Use

Write 5–8 sentences reflecting on:

-   Which R tools or approaches were most useful in this lab.
-   What worked well in your Git/gitHub workflow and team collaboration.
-   One thing you would change next time to improve your process.
-   Whether and how you used generative AI (if at all), and how it affected your understanding.

*(Write your reflection here.)*

