---
title: "Lab02: Data Manipulation and Visualization with R"
author: "TECK Squad (Formerly KTC)"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)

library(dplyr)
library(ggplot2)
library(arrow)

```

# Overview

In this lab, your team will explore the online advertising auctions data.
Your goals are to:

-   Practice **data manipulation** in R (using tidyverse).
-   Practice **data visualization** using concepts from the Data Visualization session.
-   Identify **3–4 data quality issues** that would matter in a real analysis workflow.
-   Communicate results clearly and professionally as a team.

This lab is designed to take **2–3 hours** of focused collaborative work.

> Today, you are only exploring, visualizing, interpreting, and identifying suspicious looking issues.

------------------------------------------------------------------------

# Git Workflow, Collab, and Use of Generative AI

This is a **team-based exploratory analysis**.

## Git Workflow

-   **One team member** should fork the course lab repository.
-   In the team’s cloned fork, create the `data/` folder:
-   That person should **add the rest of the team as collaborators**.
-   All team members should clone the **same shared fork** (creating an RStudio project).

To keep collaboration smooth:

-   Pull before you start working.
-   Commit frequently, with clear messages (e.g., "added price histogram", "investigated region codes").
-   Remember to pull before pushing.
-   Push your work regularly.

I will review the **Git commit history** to understand who contributed what.
Individual grades depend on **meaningful contribution** from each student.

## Team Planning (Required)

Before you begin, read the entire lab together (5–10 minutes).

-   Make sure everyone understands the goals, deliverables, and sections.

Decide how to divide the work (3–4 members).

-   As a team, choose a coordinator for each major section, and within sections split the work

    * Section 1: Initial Orientation
    * Section 2: Data Exploration (dplyr)
    * Section 3: Visualizations
    * Section 4: Synthesis & Reflection
    
-   Section coordinators will ensure their sections are completed throughout the lab's development

Create a Jira Kanban Board (team facilitator).

-   Go to Jira → Create Project → Kanban template

-   Create a board titled: “Ad Bids Lab – Team <YourTeamName>”

-   Add cards for each task/subtask, including:

    * Task name
    * Description
    * Assigned team member
    * Due date (based on team timeline)
    
Upload a screenshot (I'll check the time and date of the file) of the Kanban board.

-   A screenshot showing all initial cards in the To Do column and assignments.

-   Save it in your repository under: `planning/kanban_screenshot.png`

-   Add it **here**.
![Kanban Planning Board](planning/kanban_screenshot.png)
Make use of the team charter that you designed in Lab01, using the Kanban board throughout the project to keep you on task, correcting issues that arise and readjusting as needed.


## Use of Generative AI

You **may** use Gen.AI tools **as support**, not as a replacement.

Appropriate AI uses:

-   Understanding R errors
-   Asking for explanations of functions & syntax
-   Debugging when stuck
-   Checking whether your code is reasonable

Inappropriate uses:

-   Asking AI to complete most or all of the lab
-   Copying AI-generated code you don’t understand
-   Using AI to write your interpretations or summaries

Your code and reasoning must reflect **your own team’s understanding**.

------------------------------------------------------------------------


# Data and Setup

1.  Each team member should download from Canvas two files in `Modules → ProjectInformation`:

-   `bids_data_vDTR.parquet`

-   A **data dictionary** describing each variable

3.  In your local repo add the data and data dictionary to the `data` folder

4.  Add the data and dictionary file to your .gitignore

5.  Verify that Git is ignoring the files (in the RStudio terminal write `git status` these files should not be listed in the output)

6.  Load `tidyverse`, `ggplot2` and any other packages you find useful (`arrow`, `lubridate`, etc.)

```{r load-data}

# Load packages and data
library(tidyverse)
library(dplyr)
library(arrow)
library(ggplot2)
library(readr)
library(tibble)
library(knitr)
library(kableExtra)
library(forcats)
library(maps)
library(viridis)
library(ggrepel)
bids <- read_parquet("../data/bids_data_vDTR.parquet")

# Work with a copy called df
df <- bids

# Quick preview of the data
glimpse(df)

```

------------------------------------------------------------------------

# 1. Initial Orientation (10–15 minutes)

Answer the following in complete sentences. Use code chunks to support your answers, but write the answers in prose.

1.  How many rows and columns are in the dataset?
2.  According to the data dictionary, what does each row represent?
3.  Which variable(s) are the `key(s)` (i.e., the row identifiers)
4.  Which variables appear:
    -   numerical?
    -   categorical?
    -   suspicious or inconsistent with the dictionary?
5.  Name **two variables** you expect to matter most to advertisers and explain why.


**ANSWERS:**
  
```{r initial-orientation-dims, message=FALSE, 'hold'=TRUE}
# Dimensions
n_rows <- nrow(bids)
n_cols <- ncol(bids)

dim_table <- tibble(
  Metric = c("Nom Rows", "Nom Cols"),
             Value = c(n_rows, n_cols)
)

dim_table %>%
  kable(caption = "Dataset Dimentions") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```
  
  1.  The dataset contains **443,969 rows** and **15 columns**. 
  
  2.  According to the data dictionary, each row represents **a single line bid within a Prebid Auction**. 
     
       - The Prebid server handles online ad auctions where multiple bidders submit CPM bids.
       - Each auction can have multiple bidders and each bidder may bid on multiple sizes.
       - Each row corresponds to **one bidder's response for one size within an auction**.
  
```{r initial-orientation-keys1, message=FALSE}
# Unique values in each column
(sapply(bids, function(x) length(unique(x))))
```
  3.  There is **no single variable** in this dataset that uniquely identifies each row.
  
        - Although the data dictionary lists `AUCTION_ID`, `PUBLISHER_ID`, and `DATE_UTC` as primarily used keys for joins and grouping, none of these are unique per record.
        - The **Grain** is shown in the data dictionary as `(AUCTION_ID, bidder/seat, size, …)`, but the `bidder/seat` identifiers are **not included** in the given data subset.
        - As shown below, using just `AUCTION_ID` and `SIZE` cannot work as a a key as these are **not unique per row**, since multiple bidders can return a bid for the same size within the same auction.

```{r initial-orientation4-keys2, message=FALSE}
bids %>% 
  count(AUCTION_ID, SIZE) %>%
  filter(n > 1)
```
  
  4.  From the *data dictionary*, we expect the **numerical and categorical variables** to be:
  

```{r initial-orientation-expected, message=FALSE}
expected_types <- tibble(
  Variable = c(
    "TIMESTAMP",
    "DATE_UTC",
    "AUCTION_ID",
    "PUBLISHER_ID",
    "DEVICE_TYPE",
    "DEVICE_GEO_COUNTRY",
    "DEVICE_GEO_REGION",
    "DEVICE_GEO_CITY",
    "DEVICE_GEO_ZIP",
    "DEVICE_GEO_LAT",
    "DEVICE_GEO_LONG",
    "REQUESTED_SIZES",
    "SIZE",
    "PRICE",
    "RESPONSE_TIME",
    "BID_WON"
  ), 
  Expected_Class = c(
    "datetime",          # TIMESTAMP_NTZ
    "date",              # DATE
    "categorical",       # Auction ID
    "categorical",       # Publisher ID
    "categorical",       # desktop/mobile/tablet
    "categorical",       # 2-letter country
    "categorical",       # 2-letter USPS region
    "categorical",       # City name
    "categorical",       # ZIP / ZIP+4 string
    "numeric",           # Latitude
    "numeric",           # Longitude
    "categorical",       # List of sizes
    "categorical",       # Single size
    "numeric",           # CPM bid
    "numeric",           # Response time in ms
    "boolean"            # TRUE/FALSE winner
  ),
  Expected_Detail = c(
    "TIMESTAMP_NTZ (UTC event time)",
    "DATE (UTC calendar date)",
    "VARCHAR (auction identifier)",
    "VARCHAR (publisher/site identifier)",
    "VARCHAR, (device category, string)",
    "VARCHAR(2) (ISO 2-letter country code)",
    "VARCHAR(2) (USPS state/region postal code)",
    "VARCHAR, (city name, string)",
    "VARCHAR(10) (ZIP or ZIP+4)",
    "FLOAT (latitude of device)",
    "FLOAT (longitude of device)",
    "VARCHAR/ARRAY (all requested sizes, ex: '300x250, 320x50')",
    "VARCHAR (size for this bid, ex: `300x250')",
    "NUMBER(12,6) (CPM bid in USD)",
    "NUMBER(10,0) (bidder latency in ms)",
    "BOOLEAN (if bid won, TRUE/FALSE)"
  )
)


expected_types <- expected_types %>%
  arrange(factor(
    Expected_Class,
    levels = c("numeric", "categorical", "datetime", "date", "boolean")
  ))

expected_types %>%
  kable(caption = "Expected Variable Types from Data Dictionary") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```


We can then compare to the actual type of each variable in the data:

```{r intial-orientation-actual-compare, message=FALSE}
actual_types <- tibble(
  Variable = names(bids),
  Actual_Type = sapply(bids, class)
)

comparison <- expected_types %>%
  left_join(actual_types, by = "Variable")

comparison %>%
  kable(caption = "Expected vs Actual Types (Data Dictionary vs Raw Data)") %>%
    kable_styling(full_width = FALSE, bootstrap_options = c("striped"))
```

  Many of these are suspicious or inconsistent with the data dictionary:
  
  - **`PRICE`** and **`RESPONSE_TIME`** are expected to be *numeric*, but appear as `character` in the raw data.
  
      - This could be due to them containing *non-numeric characters* (i.e., units or extra formatting) and may need parsing before analysis.
  
  - **`DEVICE_TYPE`** is expected to be a categorical string with values like "desktop", "mobile", or "tablet", but is stored as an `integer` code.
      
      - Without a lookup table, this makes interpretations difficult t understand.
  
  - **`DEVICE_GEO_COUNTRY`** appears in the data dictionary but has no detected type in this subset (`NA` in the comparison table).
    
      - This indicates that the column is not present in our given data subset.
  
  - **`TIMESTAMP`** and **`DATE_UTC`** should be datetime/date fields, but are read in as `character`. 
  
      - These will need to be converted before our time-based analysis.

  - **`BID_WON`** is expected to be a Boolean field, but appears as `character`.
  
      - It likely needs to be re-coded to `TRUE`/`FALSE` or 0/1.

  - Beyond type mismatches, some fields like **`DEVICE_GEO_REGION`**, **`DEVICE_GEO_CITY`**, and **`DEVICE_GEO_ZIP`** may follow the correct broad class, but need to be analysed carefully for messy real-world values like **inconsistent casing**, **invalid state codes**, or **irregular ZIP formats** during our data cleaning.
  
  

  5.  **Two variables** we'd expect to matter most to advertisers are **`PRICE`** and **`SIZE`**:
  
    - **`PRICE`** represents the bidder's CPM bid. Understanding pricing patterns helps advertisers optimize bidding strategies, budget appropriately, and assess whether they are overspending or being outbid by competitors.
    
    - **`SIZE`** represents the specific ad slot dimensions they are bidding for. Different sized have different performance profiles and market values. Higher-performing or premium sizes may justify higher bids.
  

------------------------------------------------------------------------

# 2. Data Exploration Using `dplyr` (30–40 minutes)

Use functions you are comfortable with from `dplyr` and `tidyr`.
Tables and summaries should be created in code chunks; explanations should be written as text.

## 2.1 Geographic Exploration

Explore the geographical fields (such as `DEVICE_GEO_COUNTRY`, `DEVICE_GEO_REGION`, `DEVICE_GEO_CITY`, `DEVICE_GEO_ZIP`).

Tasks:

1.  Create tables with the **top 3 regions** and the **top 10 cities** by count of rows.
2.  Compare the region codes to what the data dictionary says they should look like.
3.  Identify **at least one** region code that is clearly suspicious, and explain why.

```{r geo-explore}
## Mostly utilizing the functions count() and slice_max().
## The sort=TRUE argument and slice_max() overlap in utility slightly, but 
## good to remember they both exist.

# Table with top 3 regions
bids %>% count(DEVICE_GEO_REGION, sort = TRUE) %>%
  slice_max(n, n = 3)
  
# Table with top 10 cities
bids %>%
  count(DEVICE_GEO_CITY, sort = TRUE) %>%
  slice_max(n, n = 10)

# Checking out all region codes
bids %>%
  count(DEVICE_GEO_REGION, sort = TRUE)

```

*Interpretation:*
The top 3 regions are seeming to all refer to Oregon, though the spelling is inconsistent and breaks the entries into unnecessary groupings. The top 10 cities table is well spread out, though the second most entries being connected to an NA value. When cleaning the data, we may find that the NA values can be linked to specific cities based on other variables such as Latitude, Longitude, or Zip Code, allowing this table to make more interpretive sense.

The data dictionary states the values should be a exactly two-characters, written as a USPS code. In this case, two of the codes that appear are distinctly larger than two characters (oregon, and xor). The region code I find to be clearly suspicious is xor, as it is unclear if the x is a typo or if the value is intended to be something different than Oregon. The entry "oregon" should not be an issue after the cleaning stage, as we can simply change those values to "OR".

------------------------------------------------------------------------

## 2.2 Price Behavior

Study the `PRICE` variable.

Tasks:

1.  Produce numerical summaries of `PRICE` (e.g., min, max, median, selected quantiles).
2.  Identify implausible price ranges or values and explain why they are implausible.
3.  Investigate whether suspicious prices seem to cluster by **one** of the following:
    -   publisher,
    -   region, **or**
    -   ZIP code.

Choose only one dimension and describe what you find.

```{r price-explore}
library(dplyr)

# 1) Create a cleaned numeric PRICE variable
df_price <- df %>%
  mutate(
    PRICE_num = as.numeric(PRICE),
    PRICE_num = na_if(PRICE_num, -999),   # turn -999 into NA
    PRICE_num = na_if(PRICE_num, 0)       # turn 0 into NA
  )

# 2) Numerical summary of cleaned PRICE
price_summary <- df_price %>%
  summarize(
    n_non_missing   = sum(!is.na(PRICE_num)),
    n_zero_or_neg   = sum(PRICE_num <= 0, na.rm = TRUE),
    n_very_small    = sum(PRICE_num > 0 & PRICE_num < 0.001, na.rm = TRUE),
    Min             = min(PRICE_num, na.rm = TRUE),
    Q1              = quantile(PRICE_num, 0.25, na.rm = TRUE),
    Median          = median(PRICE_num, na.rm = TRUE),
    Mean            = mean(PRICE_num, na.rm = TRUE),
    Q3              = quantile(PRICE_num, 0.75, na.rm = TRUE),
    Max             = max(PRICE_num, na.rm = TRUE)
  )

price_summary

# 3) Price by ZIP code (to see if strange prices cluster by ZIP)
zip_price <- df_price %>%
  filter(
    !is.na(PRICE_num),
    !is.na(DEVICE_GEO_ZIP),
    nchar(DEVICE_GEO_ZIP) == 5
  ) %>%
  group_by(DEVICE_GEO_ZIP) %>%
  summarize(
    n_obs        = n(),
    median_price = median(PRICE_num),
    mean_price   = mean(PRICE_num),
    max_price    = max(PRICE_num)
  ) %>%
  arrange(desc(median_price))

# Show top 10 ZIP codes by median price
head(zip_price, 10)

```

*(Write your interpretation below.)*

With a mean near 0.48 and a median of about 0.20, our numerical summaries demonstrate that the majority of bid prices are modest and positive.  Nevertheless, a non-trivial number of implausible values, including zeros, negative numbers, and incredibly low prices below 0.001, are also observed.  We treated 0, −999, and extremely low prices as problematic because these values are probably data entry or coding errors rather than actual auction prices.

Suspiciously high prices tend to concentrate in a small number of ZIP codes when prices are grouped by ZIP code.  The median and maximum prices of a small subset of ZIP codes are significantly higher than those of the entire dataset.  This implies that certain areas might have systematic problems with data quality (such as inaccurate units or misrecorded bids), and these ZIP codes ought to be thoroughly examined in subsequent cleaning or modeling stages.

------------------------------------------------------------------------
## 2.3 Response Time Behavior

Investigate the `RESPONSE_TIME` variable.

Tasks:

1.  Examine its data type and typical values.
- It takes on the character data type.
- Typical values include a "RESPONSE TIME:" delimiter, plus some minor variations. It then contains digits that represent response time in milliseconds.
2.  Look for common patterns or prefixes/suffixes in the values.
- Typical values include a "RESPONSE TIME:" delimiter.
3.  Identify an issue that would prevent straightforward numerical analysis.
- It is not of type int, and contains characters within the value. There also exists two edge cases where either the delimiter is different or trailing values beyond the digits.
```{r rt-explore}
typeof(bids$RESPONSE_TIME)
filter(.data=bids, substr(RESPONSE_TIME, 1, 14) != "RESPONSE_TIME:")
filter(.data=bids, grepl("^[0-9]$", substr(RESPONSE_TIME, nchar(RESPONSE_TIME), nchar(RESPONSE_TIME))) == FALSE)
```

*(Write your interpretation below.)*
There is clear sign of tampering given that there is one off inconsistencies in the delimiter and the trailing characters after the digits. Otherwise, easily reversible.

------------------------------------------------------------------------

# 3. Visualization Challenges (45–60 minutes)

Create **three** high-quality visualizations using `ggplot2`.

Each figure must include:

-   A clear and professional title
-   Labeled axes
-   A readable theme (e.g., `theme_bw()`, `theme_minimal()`)
-   Thoughtful design choices (color, faceting, ordering, etc.)
-   A **3–5 sentence interpretation**

## 3.1 Plot 1 — Distribution of Bid Prices

Create a visualization that reveals:

-   The overall shape of the price distribution
-   Outliers or impossible values
-   Any issues you suspect based on the data dictionary

```{r plot-price-1}
library(ggplot2)
idx <- which(substr(bids$PRICE, 1, 1)=="O")
bids$PRICE[idx][1] <- "0"
altered_price_data <- mutate(.data=bids, PRICE=as.double(PRICE))
rm(idx)
altered_price_data <- group_by(.data=altered_price_data, PRICE)
altered_price_data <- summarize(.data=altered_price_data, Count=n())
ggplot(altered_price_data, aes(x = PRICE, y = Count)) + geom_point() + ggtitle("Price Histogram (with outliers)") + labs(x="Price") + theme_bw() # With outliers
```

```{r plot-price-2}
altered_price_data_1 <- filter(.data=altered_price_data, PRICE < 25, PRICE > 0)
ggplot(filter(.data=altered_price_data_1 , PRICE < 25, PRICE > 0), aes(x = PRICE)) + geom_histogram(aes(y = ..density..), bins = 200, fill = "lightblue") + stat_function(fun = dexp, args = list(rate = (1 / mean(altered_price_data_1 $PRICE))), color = "red", size = 1) + ggtitle("Price Distribution (Exponential) (without outliers)") + labs(x = "Price", y = "Density") + theme_bw() # Without outliers
rm(altered_price_data_1)
```

```{r plot-price-3}
altered_price_data_2 <- filter(.data=altered_price_data, PRICE> 0, PRICE < 5)
ggplot(altered_price_data_2, aes(x = PRICE)) + geom_histogram(aes(y = ..density..), bins = 200, fill = "lightblue") + stat_function(fun = dexp, args = list(rate = (1 / mean(altered_price_data_2$PRICE))), color = "red", size = 1) + ggtitle("Price Distribution (Exponential) (without outliers and further zoomed in)") + labs(x = "Price", y = "Density") + theme_bw() # More zoomed in approach
rm(altered_price_data_2, altered_price_data)
```
*Interpretation (3–5 sentences):*
The most apparent outliers exist in the region where the PRICE is less than -5 and have been removed due to the irretrievable nature of the values of those rows. It has been previously clarified (between me and Adam) that the values that exist between -5 and 0 are rows which have had their PRICE multiplied by -1 (which means that they can be retrieved). Furthermore, we have values above 25 which may not be outliers, but exist in such small quantities that they might as well not exist. Similarly, values beyond 5 can be considered to be negligible. However, given that they aren't inherently deemed outliers, there are two distribution density plots provided which include prices below 25 and below 5 respectively. From the initial histogram, it can be seen that the PRICE roughly follows an exponential distribution, and the distribution density plots in turn slightly reinforce this claim.

------------------------------------------------------------------------

## 3.2 Plot 2 — Price by Zip City

Compare a central price measure (mean or median) across cities

Guidance:

-   Be explicit about whether you use mean or median.
-   Consider sorting cities by the measure you plot.
-   Highlight any cities whose price behavior appears inconsistent or suspicious.

For our plot, we care about:

  - `PRICE` (central measure by city)
  - `DEVICE_GEO_CITY` (grouping)
  - `DEVICE_GEO_LAT`, `DEVICE_GEO_LONG` (for map positions)

We first explore these variables to look for suspicious behavior:

```{r price-summary, message=FALSE}
price_summary <- bids %>%
  mutate(PRICE_num = parse_number(PRICE)) %>%
  summarise(
    n_rows       = n(),
    min_price    = min(PRICE_num, na.rm = TRUE),
    max_price    = max(PRICE_num, na.rm = TRUE),
    n_negative   = sum(PRICE_num < 0, na.rm = TRUE),
    n_eq_neg999  = sum(PRICE_num == -999, na.rm = TRUE)
  )

price_summary %>%
  kable(caption = "Summary of PRICE values before cleaning") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```


  - We see that there are **negative prices**, including a special value of **-999**, so in the cleaning step below we will treat **-999** as missing and convert the remaining negative prices to positive values (assuming they are sign errors). 
  - We also see a questionably high price for the *max*. Due to the possibility of extreme outliers here, we will use the **median** as a more robust measure of central tendency for our city-level comparison.
  
  
Using Oregon’s approximate bounds (according to "MapsofTheWorld.com"):

- Latitude: **$42^\circ$ N to $46.15^\circ$ N**  
- Longitude: **$-124.3^\circ$ W to $-116.45^\circ$ W**

```{r lat-long-summary, message=FALSE}
# Added slight buffers to values
latlong_summary <- bids %>%
  summarise(
    n_rows          = n(),
    min_lat         = min(DEVICE_GEO_LAT, na.rm = TRUE),
    max_lat         = max(DEVICE_GEO_LAT, na.rm = TRUE),
    n_lat_outside   = sum(DEVICE_GEO_LAT < 41    | DEVICE_GEO_LAT > 47,     na.rm = TRUE),
    min_long        = min(DEVICE_GEO_LONG, na.rm = TRUE),
    max_long        = max(DEVICE_GEO_LONG, na.rm = TRUE),
    n_long_outside  = sum(DEVICE_GEO_LONG < -125 | DEVICE_GEO_LONG > -116, na.rm = TRUE),
    n_long_10_over  = sum(DEVICE_GEO_LONG < -121 & DEVICE_GEO_LONG > -127.5, na.rm = TRUE)
  )

latlong_summary %>%
  kable(caption = "Lat/long values compared to approximate Oregon bounds") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```
  
  
  - Exactly 100 of rows have **longitudes** outside Oregon’s expected range (exact 100 implies an added "bomb"). 
      - We also see that the majority of the points lie about 10 degrees over from these shifted values (tested by checking number of points in small range about 10 degrees away).
      - However, we cannot be certain that this was the exact shift and since it is only 100 points, we will omit from analysis.

```{r city-summary, message=FALSE}
city_summary <- bids %>%
  mutate(
    PRICE_num = readr::parse_number(PRICE),
    CITY = if_else(is.na(DEVICE_GEO_CITY), "Unknown", DEVICE_GEO_CITY)
  ) %>%
  group_by(CITY) %>%
  summarise(
    n            = n(),
    median_price = median(PRICE_num, na.rm = TRUE),
    .groups      = "drop"
  ) %>%
  arrange(desc(n)) %>%
  slice_head(n = 10)

city_summary %>%
  kable(caption = "Top 10 cities by count with median price (pre-cleaning)") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped"))
```

  - We see the second largest group is **"Unknown"** (or `NA` in raw data).
      - Since we are not sure if `NA`/"Unknown" represents multiple underlying locations, we cannot interpret it meaningfully at the city level and we will omit from this visualization.
      
      
We now implement the necessary changes found above and prepare for our plot:

```{r plot-prep, message=FALSE}
bids_plot <- bids %>%
  mutate(
    # Numeric price for analysis
    PRICE_num = parse_number(PRICE),
    # Treat -999 as missing
    PRICE_num = if_else(PRICE_num == -999, NA_real_, PRICE_num),
    # Flip remaining negatives to positive (assumed sign errors)
    PRICE_num = if_else(PRICE_num < 0 & !is.na(PRICE_num),
                        -PRICE_num, PRICE_num),
    # City label for plotting
    CITY = if_else(is.na(DEVICE_GEO_CITY), "Unknown", DEVICE_GEO_CITY)
  ) %>%
  # Drop rows with missing price or coords
  filter(
    !is.na(PRICE_num),
    !is.na(DEVICE_GEO_LAT),
    !is.na(DEVICE_GEO_LONG)
  ) %>%
  # Keep only points inside (buffered) Oregon bounds
  filter(DEVICE_GEO_LONG >= -125, DEVICE_GEO_LONG <= -116) %>%
  # Drop "Unknown" city for city-level comparison
  filter(CITY != "Unknown")

# City-level summary for plotting
city_price_map <- bids_plot %>%
  group_by(CITY) %>%
  summarise(
    n            = n(),
    median_price = median(PRICE_num, na.rm = TRUE),
    lat          = median(DEVICE_GEO_LAT, na.rm = TRUE),
    long         = median(DEVICE_GEO_LONG, na.rm = TRUE),
    .groups      = "drop"
  ) %>%
  filter(n >= 100)        # Cities with reasonably large sample size

# Ranges for zooming the map
lat_range  <- range(city_price_map$lat)
long_range <- range(city_price_map$long)
```


```{r plot-price-by-city, message=FALSE, warning=FALSE}
# Load U.S. states map and keep only Oregon
us_states <- map_data("state")
oregon_map <- us_states %>% filter(region == "oregon")

# Top cities to label (by bid count)
label_cities <- city_price_map %>%
  slice_max(n, n = 7)

# Size breaks for legend (so it doesn't get huge)
size_breaks <- c(100, 1000, 10000)

ggplot() +
  # Background of Oregon
  geom_polygon(
    data = oregon_map,
    aes(x = long, y = lat, group = group),
    fill  = "azure3",
    color = "gray40",
    linewidth = 1
  ) +
  # City points with median price & count
  geom_point(
    data = city_price_map,
    aes(x = long, y = lat,
        color = median_price,
        size  = n),
    alpha = 0.95
  ) +
  # Labels for highest-volume cities
  geom_text_repel(
    data = label_cities,
    aes(x = long, y = lat, label = CITY),
    size = 4.5,
    max.overlaps = 20,
    fontface = "bold",
    color = "black",
    bg.color = "lightcyan1",
    bg.r = 0.1,
    nudge_x = 1,
    direction = "y",
    hjust = 0,
    segment.color = "gray20",
    segment.size = 0.6
    
  ) +
  scale_color_viridis_c(
    option = "plasma",
    name   = "Median CPM"
  ) +
  scale_size_continuous(
    name  = "Bid Count",
    range = c(2, 18),
    breaks = size_breaks,
    labels = scales::comma(size_breaks),
    guide = guide_legend(override.aes = list(size = c(3, 6)))
  ) +
  coord_fixed(
    1.3,
    xlim = long_range,
    ylim = lat_range
  ) +
  labs(
    title = "Median Bid Price by City",
    subtitle = "Oregon Cities (at least 100 bids)",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.background = element_rect(fill = "grey90", color = NA),

    panel.grid = element_line(color = "lightblue3"),
    legend.position = "left",
    plot.title.position = "plot",
    plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
    plot.subtitle = element_text(size = 13, hjust = 0.5)
  ) 
```

**Interpretation:**

This plot shows the **median bid price (CPM)** for ad auction across Oregon cities with at least 100 bids. 

  - Prices cluster tightly across most cities, with typical medians falling between **\$0.15 - \$0.30**, suggesting fairly consistent bidding behavior across the region.
  - Higher-volume markets (like **Portland**, **Hilsboro**, **Beaverton**, **Salem**, and **Medford**) do not show systematically higher prices and both the highest and lowest median CPM values are seen throughout the state in lower-volume markets. This indicates that auction prices are not strictly tied to city size.
  - A few cities exhibit noticeably higher or lower medians than their neighbors, which may reflect **localized data quality issues**, **market anomolies**, or **bidder strategy differences**.
  - Overall, no extremely or structurally suspicious patterns appear after cleaning negative and invalid values and filtering for cities with "reasonable" amount of bids ($>100$).


------------------------------------------------------------------------

## 3.3 Plot 3 — Analyst’s Choice

Choose a visualization that you believe would be useful to a data science team at an advertising company.

Possible ideas:

-   Auction volume by hour of day
-   Differences between winning and losing bids
-   ZIP code irregularities
-   Response time patterns
-   Publisher-level differences

Your plot should answer a clear question, and your interpretation should explain what you learned.


```{r plot-analyst-choice}

library(dplyr)
library(ggplot2)

# Create cleaned numeric price column
df_price <- df %>%
  mutate(
    PRICE_num = suppressWarnings(as.numeric(PRICE)),
    PRICE_num = ifelse(PRICE_num < 0 | PRICE_num > 200, NA, PRICE_num)
  )

# Summarize price by device type
device_price <- df_price %>%
  filter(!is.na(PRICE_num), !is.na(DEVICE_TYPE)) %>%
  group_by(DEVICE_TYPE) %>%
  summarize(
    n_obs        = n(),
    median_price = median(PRICE_num),
    mean_price   = mean(PRICE_num)
  ) %>%
  arrange(desc(median_price))

# Plot median price by device type
ggplot(device_price,
       aes(x = reorder(DEVICE_TYPE, median_price),
           y = median_price)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Median Bid Price by Device Type",
    x = "Device Type",
    y = "Median Price"
  ) +
  theme_minimal()

```

*Interpretation (3–5 sentences):*
The median bid price for various device types is displayed in this graphic.  Stronger advertiser demand or higher-value impressions on particular platforms are indicated by the significantly higher median prices of some device categories.  In the meantime, a number of device types exhibit extremely low median values, which may indicate different auction dynamics or lower-quality traffic.  These variations suggest that device type should be taken into account in downstream modeling since it has a significant impact on auction pricing.

```{r Device vs Response Time plot}
## This is a boxplot that represents Response Time by Device Type.
bids %>%
  mutate(RESPONSE_TIME = parse_number(RESPONSE_TIME)) %>% # Removing chars from RESPONSE_TIME variable
  mutate(DEVICE_TYPE = factor(DEVICE_TYPE)) %>% # Setting DEVICE_TYPE as a factor
  ggplot(aes(x = DEVICE_TYPE, y = RESPONSE_TIME, fill = DEVICE_TYPE)) + # Plotting the BoxPlot and labels
  geom_boxplot(outlier.size = 0.7) +
  labs(
    title = "Response Time by Device Type",
    x = "Device Type",
    y = "Response Time (ms)"
  )

```

*Interpretation:*
Does Response Time vary based on the type of device? This box plot depicts bidder response latency across the various device types in the data. The device type variable is mislabeled, causing the specific categories to be unidentified. We can see, however, that the median response values for each device are closely grouped, with devices 4 and 5 having slightly slower medians than the others. Device 2 has a wider inner quartile range, showing that its values have more spread around the median. Additionally, device 4 shows a significant number of outlier values much higher than the other 4 devices. This could imply that, while the median is still low, there is some issue of latency for that device. If we can correctly identify which category corresponds to which device, this boxplot implies there could be differences between the devices with regard to response time.


------------------------------------------------------------------------

# 4. Synthesis & Reporting (20–25 minutes)

## 4.1 Data Quality Issues

Based on your work above, list **3–4** data quality issues you discovered.
For each issue:

-   Name the affected variable(s).
-   Describe what the issue is.
-   Provide evidence (e.g., from a summary, table, or plot).
-   Suggest one way you would address it in a later **data cleaning** lab.

*(Write your answer here in paragraphs or bullet points.)*
- PRICE: Negative outlier (-999) values. 
Shown by first price distribution plot.
A fix involves removing these values entirely.
- PRICE: Negative price (between -5 and 0) values.
Shown by first price distribution plot.
A fix involves multiplying the negative values where PRICE is between -5 and 0 by -1.
- RESPONSE_TIME: One-off inconsistent delimiter and one-off trailing characters after digits.
Shown by 2.3.
A fix involves correct delimiter and removing the trailing digits.

------------------------------------------------------------------------

## 4.2 Implications for Analytics

In 1–2 paragraphs, discuss how the issues you identified could affect:

-   Business decisions,
-   Reporting to advertisers, and/or
-   Predictive modeling.

Which issues are most critical to fix before building models, and why?

*(Write your answer here.)*
Downstream analytics are significantly impacted by the data quality problems found in PRICE, including zeros, negative values, -999 codes, and incredibly small, unrealistic prices.  These values would skew model estimates, skew summary statistics, and produce deceptive patterns if they were not adjusted, particularly in tasks like campaign performance evaluation, price prediction, and optimization.

Inconsistencies at the ZIP and city levels may also indicate that some areas have different units of measurement or systematic issues with data entry.  This implies that if these problems are not resolved, spatial or regional models may yield unreliable results.  In general, validating price ranges, eliminating impossible values, and guaranteeing consistency across geographic fields are the most important tasks prior to modeling.  Resolving these problems enhances the validity of any analytical conclusions drawn from the dataset.
------------------------------------------------------------------------

## 4.3 Reflection: Tools, Workflow, AI Use

Write 5–8 sentences reflecting on:

-   Which R tools or approaches were most useful in this lab.
-   What worked well in your Git/gitHub workflow and team collaboration.
-   One thing you would change next time to improve your process.
-   Whether and how you used generative AI (if at all), and how it affected your understanding.

**Chris:**
1) I enjoyed utilizing the pipe operator combined with the mutate() function to achieve what I needed without needing to alter the original dataset. The parse_number() function was also a very handy addition, specifically for the variables I was working with.  
2) Having a structured way to answer the questions, as well as a plan to segment out the workload fairly felt easier once it was assigned.   
3) I mistakenly did not look at the variable types before attempting to create the ResponseTime v. DeviceType graph, so next time a more thorough look at the data will come first.  
4) I used generative AI lightly to understand where my issues coding my graph were occurring. It didn't end up helping until I stopped and checked the data, which is when my issues made sense and I could fix them.

**Esther:**
 In this lab, the most useful R tools were the *tidyverse* workflow, particularly `dplyr`, for cleaning suspicious values and `ggplot2`
for building layered visualizations. Using `parse_number()`, `mutate()`, and grouped summaries made it straight-forward to diagnose issues like negative prices and shifted longitudes.
Our Git/GitHub workflow continues to progress, with the team understanding and skillsets utilizing branching, pull requests, and reviewer assignments continuing to expand as we progress thorugh this assignment.
One thing I would improve next time is to have decided on setting up a separate Kanban for this lab earlier on, as this would have helped us track our responsibilities and progress more clearly from the beginning. 
(However, I think a lot of our work for this lab was pushed closer to this lab's due date due to other work & mideterms, project work, and preparation for the upcoming holiday.)

I used generative AI to help with debugging my plots (I had one line overriding another in my more complicated "Median Price by City" `ggplot2`.). This did not replace understanding, but helped me get through the tedious process of debugging a `ggplot2` plot more easily. 
On the contrary, I ended up learning a lot of interesting tricks through my attempts at using the AI to find my error.

**Takeshi**
I believe the functions/methods filter, summarize and mutate were the most useful in the process of identifying errors. For the more complicated errors, utilizing more niche functions provided great support in my exploration for these errors. The overall structure of commiting and putting in a pull request has been beneficial to keep the main branch looking clean. I would spend more time studying the functions on other data sets if possible to familiarize myself with them more. I did use generative AI (mostly chatGPT) in order to query the existence of certain functions that I would of otherwise had to code around if they had not existed. Furthermore, I had utilized chatGPT to generate formats for which I plotted data on. Personally, I believe that the use of AI had sped up the process of completing this lab, but I do believe that struggling a little more may have been beneficial to a greater understanding and ability to recall the certain existances of functions (I found myself sometimes asking the same question, but for the most part, I found myself relying on AI less later in the process of doing the main project, and this lab).

**Khloud:**
Using ggplot2 for visualization and dplyr for data cleaning proved to be very beneficial in streamlining my workflow during this lab.  The process felt organized and doable after loading the dataset, carefully cleaning each variable, and then making targeted plots. At first, using Git and GitHub was difficult, particularly when it came to branches and pull requests. However, working with my team prevented conflicts and improved the workflow.
Throughout the lab, I encountered a number of challenges, and I frequently relied on my teammates to identify mistakes and resolve problems, particularly when handling file paths, Git commands, and inconsistent data.  Their assistance was crucial to my progress.  Additionally, I used generative AI as a learning tool to help me understand R functions, explain error messages, and walk me through difficult steps—not to write the analysis for me.  My comprehension and confidence when using the tools were enhanced by the combination of teamwork and AI support.
In order to solve issues more quickly and move through the workflow more easily, I would like to practice Git and R more independently in the future.

